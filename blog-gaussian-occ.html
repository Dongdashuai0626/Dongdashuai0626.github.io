<!DOCTYPE html>
<html lang="zh-cn">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GaussianOcc: Fully Self-supervised and Efficient 3D Occupancy Estimation with Gaussian Splatting | Jiacheng Dong</title>
    <link rel="stylesheet" href="css/style.css">
    <!-- MathJax configuration -->
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                ignoreHtmlClass: 'tex2jax_ignore',
                processHtmlClass: 'tex2jax_process'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" id="MathJax-script"></script>
    <script>
        document.addEventListener('DOMContentLoaded', () => {
            const navToggle = document.querySelector('.nav-toggle');
            
            if (navToggle) {
                navToggle.addEventListener('click', () => {
                    document.querySelector('.nav-links').classList.toggle('active');
                });
            }
        });
    </script>
</head>
<body>
    <!-- Navigation -->
    <header>
        <nav class="container">
            <a href="index.html" class="logo">Jiacheng Dong</a>
            <button class="nav-toggle">Menu</button>
            <ul class="nav-links">
                <!-- Main page sections -->
                <li class="nav-section">
                    <a href="index.html#about">About</a>
                    <a href="index.html#news">News</a>
                    <a href="index.html#research">Research</a>
                    <a href="index.html#experience">Experience</a>
                    <a href="index.html#education">Education</a>
                    <a href="index.html#awards">Awards</a>
                    <a href="index.html#contact">Contact</a>
                </li>
                <!-- External pages -->
                <li class="nav-external">
                    <a href="hobbies.html">Hobbies</a>
                    <a href="blog.html" class="active">Blog</a>
                </li>
            </ul>
        </nav>
    </header>

    <div class="container">
        <a href="blog.html" class="back-to-blog">← 返回博客首页</a>
        
        <article class="article">
            <header class="article-header">
                <h1 class="article-title">GaussianOcc: Fully Self-supervised and Efficient 3D Occupancy Estimation with Gaussian Splatting</h1>
                <div class="article-meta">
                    <span>发布时间：2025年8月2日</span> | <span>作者：董嘉铖</span>
                </div>
            </header>
            
            <div class="article-content">
                <h2>问题/背景</h2>

                <p>三维语义占据预测 (3D Semantic Occupancy Prediction) 是自动驾驶和机器人领域中一项至关重要的感知任务，其目标是理解车辆周围三维空间的几何结构和每个部分的语义类别（如汽车、道路、建筑等）。这项技术为系统提供了全面的空间认知，是实现高级别自主能力的基础。</p>

                <p>然而，现有的方法面临三大核心挑战：</p>

                <ul>
                    <li><strong>对标注数据的严重依赖</strong>：许多性能领先的方法需要大量精细的三维体素级别标注，这些标注的获取成本高昂且耗费人力。</li>
                    <li><strong>计算效率和可扩展性问题</strong>：主流方法通常采用密集的体素网格来表示三维场景，这导致了巨大的计算和内存开销。此外，基于体渲染（volume rendering）的自监督方法虽然减少了对三维标签的依赖，但渲染过程本身效率低下。</li>
                    <li><strong>对传感器真值的依赖</strong>：许多自监督方法在训练时，仍然需要来自传感器的高精度地面实况（Ground Truth）6D 自我位姿（ego pose）来计算几何约束，这限制了其在无真值位姿数据下的应用。</li>
                </ul>

                <h2>方法</h2>

                <h3><strong>第一阶段：通过高斯"拍平"进行尺度感知训练 (Scale-aware Training)</strong></h3>

                <p>这一阶段的目标是</p>

                <p><strong>联合训练一个深度估计网络和一个 6D 位姿网络，从而摆脱对地面实况 6D 位姿的依赖</strong>。</p>

                <ul>
                    <li><strong>核心模块：用于投影的高斯"拍平" (Gaussian Splatting for Projection, GSP)</strong>
                        <p><strong>原理</strong>：利用环视摄像头之间存在的重叠视角区域来提供尺度信息。如果模型的深度预测和位姿预测是准确的，那么从一个视角（源视角）的图像重建出的三维场景，再渲染到相邻的另一个视角（目标视角）时，其渲染图像应与目标视角的真实图像一致。</p>
                        <ul>
                            <li><strong>实现方式</strong>：
                                <ol>
                                    <li>模型不仅预测深度图，还预测每个像素对应的 2D 高斯属性，如缩放和旋转。</li>
                                    <li>将这些 2D 高斯属性连同深度值一起反向投影（Unprojection）到三维空间，形成一组三维高斯函数。</li>
                                    <li><strong>关键步骤</strong>：在渲染前，对相邻视图重叠区域的一侧进行<strong>遮挡（Mask-out）</strong>。这样做是为了强制模型必须依赖另一侧视图的信息来重建被遮挡的区域，从而有效地学习场景的真实尺度。如果没有这个遮挡，模型会"作弊"，直接从当前视图复制信息，导致学习失败。</li>
                                    <li>通过高斯"拍平"渲染器，将被遮挡后的三维高斯函数渲染到相邻视图，并与真实图像计算
                                        <strong>跨视图光度损失 ($\mathcal{L}_\text{cross}$)</strong>。这个损失函数会同时优化深度网络和 6D 位姿网络。<strong>跨视图光度损失 ($\mathcal{L}_\text{cross}$)</strong>。这个损失函数会同时优化深度网络和 6D 位姿网络。</li>
                                </ol>
                            </li>
                        </ul>
                    </li>
                </ul>

                <h3><strong>第二阶段：通过高斯"拍平"进行快速渲染 (Fast Rendering)</strong></h3>

                <p>在第一阶段获得了无需真值的、高质量的 6D 位姿后，第二阶段的目标是</p>

                <p><strong>高效地进行自监督三维占据估计</strong>。</p>

                <ul>
                    <li><strong>核心模块：来自体素空间的高斯"拍平" (Gaussian Splatting from Voxel, GSV)</strong>
                        <p><strong>动机</strong>：传统的体渲染需要对每个像素发出的光线进行密集采样，计算量巨大且存在冗余。例如，在 OccNeRF 中，采样点数可达上亿个，而最终优化的目标——体素网格顶点——只有约两百万个。</p>
                        <ul>
                            <li><strong>实现方式</strong>：
                                <ol>
                                    <li>GSV 将三维占据场中的
                                        <strong>每个体素顶点直接视为一个三维高斯函数的位置 ($\mathcal{X}$)</strong>。</li>
                                    <li>由于这些顶点的位置是固定的，模型只需优化每个高斯的其余属性，主要是
                                        <strong>语义信息和不透明度 (Opacity)</strong>。<strong>语义信息和不透明度 (Opacity)</strong>。</li>
                                    <li>为了简化，每个体素顶点高斯的
                                        <strong>缩放 ($s$) 和旋转 ($r$) 可以设为固定的</strong>。对于空旷区域，网络会学着将其对应顶点的不透明度预测为零，使其在渲染中不可见。<strong>缩放 ($s$) 和旋转 ($r$) 可以设为固定的</strong>。对于空旷区域，网络会学着将其对应顶点的不透明度预测为零，使其在渲染中不可见。</li>
                                </ol>
                            </li>
                        </ul>
                        <p><strong>优势</strong>：这种方法用一次性的高斯"拍平"渲染代替了密集的射线采样，极大地提高了效率。实验证明，其渲染速度比体渲染快 5 倍，训练速度快 2.7 倍。</p>
                    </li>
                </ul>

                <h3><strong>损失函数</strong></h3>

                <ul>
                    <li><strong>第一阶段损失 $\mathcal{L}_\text{stage1}$</strong>：由时间光度损失（Temporal View Loss）和跨视图光度损失（Cross-View Loss）组成。</li>
                    <li><strong>第二阶段损失 $\mathcal{L}_\text{stage2}$</strong>：由时间光度损失和 2D 语义损失（Semantic Loss）组成。</li>
                </ul>
            </div>
        </article>
    </div>

    <!-- Footer -->
    <footer>
        <div class="container">
            <div class="footer-links">
                <a href="mailto:Dong48@illinois.edu">Email</a>
                <a href="https://github.com/dongjiacheng06" target="_blank">GitHub</a>
                <a href="index.html">主页</a>
                <a href="blog.html">博客</a>
            </div>
            <div>&copy; 2025 Jiacheng Dong. All rights reserved.</div>
            <div style="margin-top:0.5rem; font-size:0.9rem; opacity:0.8;">
                Personal Website: <a href="https://dongjiacheng06.github.io" target="_blank" style="color:#fff;">dongjiacheng06.github.io</a>
            </div>
        </div>
    </footer>
</body>
</html>