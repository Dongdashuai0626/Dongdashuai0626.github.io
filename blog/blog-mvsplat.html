<!DOCTYPE html>
<html lang="zh-cn">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images | Jiacheng Dong</title>
    <link rel="stylesheet" href="../assets/css/main.css">
    <!-- MathJax configuration -->
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                ignoreHtmlClass: 'tex2jax_ignore',
                processHtmlClass: 'tex2jax_process'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" id="MathJax-script"></script>
    <!-- JavaScript -->
    <script src="../assets/js/main.js" defer></script>
</head>
<body>
    <!-- Navigation -->
    <header class="header">
        <nav class="nav container">
            <a href="../index.html" class="logo" aria-label="Jiacheng Dong's Homepage">Jiacheng Dong</a>
            <button class="nav-toggle" aria-label="Toggle navigation menu" aria-expanded="false">
                <span class="hamburger"></span>
                <span class="hamburger"></span>
                <span class="hamburger"></span>
            </button>
            <ul class="nav-links" role="navigation">
                <!-- Main page sections -->
                <li class="nav-section" role="none">
                    <a href="../index.html#about">About</a>
                    <a href="../index.html#news">News</a>
                    <a href="../index.html#research">Research</a>
                    <a href="../index.html#experience">Experience</a>
                    <a href="../index.html#education">Education</a>
                    <a href="../index.html#awards">Awards</a>
                    <a href="../index.html#contact">Contact</a>
                </li>
                <!-- External pages -->
                <li class="nav-external" role="none">
                    <a href="../hobbies.html">Hobbies</a>
                    <a href="../photography.html">Photography</a>
                    <a href="../blog.html" class="active" aria-current="page">Blog</a>
                </li>
            </ul>
        </nav>
    </header>

    <div class="container">
        <a href="../blog.html" class="back-to-blog">← 返回博客首页</a>
        
        <article class="article">
            <header class="article-header">
                <h1 class="article-title">MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images</h1>
                <div class="article-meta">
                    <span>发布时间：2025年8月2日</span> | <span>作者：董嘉铖</span>
                </div>
            </header>
            
            <div class="article-content">
                <h2>问题/背景</h2>

                <p>传统的从图片生成3D场景的方法（如NeRF或3DGS）通常需要大量的输入图片（例如上百张）和漫长的"逐场景优化"过程，这在实际应用中非常不便 。近年来出现了一些"前馈"(feed-forward) 模型，它们通过在大型数据集上预训练，可以快速地对新场景进行3D重建，无需再次优化 。</p>

                <p>然而，即便是最新的前馈模型（如论文中反复对比的</p>

                <p>`pixelSplat`），在仅有两三张输入图片的情况下，也很难准确地重建出场景的3D几何结构 。</p>

                <p>`pixelSplat` 直接从图像特征中推断深度的概率分布，这种方式较为模糊，容易产生有噪点和悬浮物的3D模型 。</p>

                <h2>方法</h2>

                <p><img src="../paper_notes/image%207.png" alt="image.png"></p>

                <p>MVSplat 的核心目标是学习一个前馈网络fθ，该网络接收 K 个稀疏视角的图像 {Ii} 及其对应的相机内外参矩阵 {Pi} 作为输入，然后直接输出一组完整的3D高斯基元参数 。这些参数包括中心位置μj、不透明度 αj、协方差 Σj 和以球谐函数表示的颜色 cj 。</p>

                <p>整个方法可以分为两个主要部分：<strong>多视角深度估计</strong> 和 <strong>高斯参数预测</strong>。</p>

                <ol>
                    <li>多视角深度估计 (Multi-View Depth Estimation)</li>
                </ol>

                <p>这是 MVSplat 方法的基石，目的是为了精准地预测 3D 高斯基元的中心位置μj 。该深度模型完全基于 2D 卷积和注意力机制，避免了计算开销大的 3D 卷积，因此非常高效 。它包含以下几个步骤：</p>

                <ul>
                    <li><strong>多视角特征提取 (Multi-view feature extraction)</strong>
                        <ul>
                            <li>首先，使用一个浅层的类 ResNet 的 CNN 网络对每张输入图像进行特征提取，得到被 4 倍下采样的特征图 。</li>
                            <li>然后，将这些特征图送入一个带有自注意力和交叉注意力层的多视角 Transformer 中 。这里采用的是 Swin Transformer 的局部窗口注意力机制以提高效率 。</li>
                            <li>交叉注意力机制使得每个视图都能与其他所有视图交换信息，从而得到富含跨视图信息的特征{Fi} 。</li>
                        </ul>
                    </li>
                </ul>

                <p><img src="../paper_notes/image%208.png" alt="image.png"></p>

                <ul>
                    <li><strong>代价体构建 (Cost-volume-construction)</strong>
                        <ul>
                            <li>这是方法的核心，通过"平面扫描（plane-sweep）"的方式来构建代价体，以编码不同深度候选值的跨视图特征匹配信息 。</li>
                            <li>模型在预设的最近和最远深度范围之间，在逆深度域中均匀采样 D 个深度候选值{dm} 。</li>
                            <li>对于参考视图 i，模型会将其他视图 j 的特征Fj 根据每个深度候选值 dm "变换"或"扭曲"到视图 i 的视角下，得到 D 个扭曲后的特Fdmj→i</li>
                            <li>接着，计算参考视图 i 的原始特征Fi 和每个扭曲后的特征 Fdmj→i 之间的点积相似度（相关性），得到 D 个相关性图 Cdmi 。</li>
                            <li>将所有 D 个相关性图堆叠起来，就构成了视图 i 的代价体Ci∈R4H×4W×D 。当有超过两个输入视图时，会计算所有其他视图与参考视图 i 的相关性，然后按像素取平均值 。</li>
                        </ul>
                    </li>
                    <li><strong>代价体优化 (Cost volume refinement)</strong>
                        <ul>
                            <li>由于在纹理较少的区域，初始代价体可能存在歧义，因此模型使用一个轻量级的 2D U-Net 来对其进行优化 。</li>
                            <li>U-Net 的输入是 Transformer 特征和初始代价体Ci 的拼接 。它输出一个残差ΔCi，加到初始代价体上得到优化后的代价体 C~i 。</li>
                            <li>为了在不同视图的代价体之间交换信息，U-Net 在其最低分辨率的层中加入了交叉视图注意力层 。</li>
                        </ul>
                    </li>
                    <li><strong>深度估计 (Depth estimation)</strong>
                        <ul>
                            <li>优化后的代价体C~i 会被一个基于 CNN 的上采样器恢复到全分辨率 C^i 。</li>
                            <li>模型对全分辨率代价体C^i 在深度维度上应用 softmax 操作，将其归一化为概率分布 。</li>
                            <li>最后，通过对所有深度候选值进行加权平均，计算出最终的深度图Vi 。</li>
                        </ul>
                    </li>
                    <li><strong>深度优化 (Depth refinement)</strong>
                        <ul>
                            <li>为了进一步提升性能，模型引入了一个额外的深度优化步骤 。</li>
                            <li>一个非常轻量级的 2D U-Net 会接收多视角图像、特征和当前预测的深度图作为输入，输出一个残差深度图 。</li>
                            <li>将这个残差深度加到当前深度上，得到最终的深度输出 。这个U-Net同样在低分辨率层加入了交叉视图注意力 。</li>
                        </ul>
                    </li>
                </ul>

                <h3>2. 高斯参数预测 (Gaussian Parameters Prediction)</h3>

                <p>在得到高精度的深度图后，模型会并行地预测高斯基元的其他参数：</p>

                <ul>
                    <li><strong>高斯中心 μ</strong>: 直接将多视角深度图利用相机参数反投影到三维世界坐标系中，形成点云 。然后将所有视图的点云合并，这些合并后的点云就作为 3D 高斯基元的中心 。</li>
                    <li><strong>不透明度 α</strong>: 在深度估计的 softmax 操作后，可以得到每个像素的匹配置信度（即 softmax 输出的最大值） 。这个置信度的物理意义与不透明度很相似（高置信度意味着该点很可能在物体表面），因此模型用两个卷积层从匹配置信度中预测不透明度α 。</li>
                    <li><strong>协方差 Σ 和颜色 c</strong>: 这两个参数由另外两个卷积层预测得出，其输入是图像特征、优化后的代价体和原始多视角图像的拼接 。协方差矩阵由一个缩放矩阵和一个旋转矩阵构成，颜色 c 则由预测出的球谐函数系数计算得到 。</li>
                </ul>

                <h3>3. 训练损失 (Training Loss)</h3>

                <p>MVSplat 的整个模型是端到端训练的，仅使用渲染图像和真实目标图像之间的光度损失（photometric loss）作为监督信号，无需任何真实的几何（如深度图）监督 。训练损失是</p>

                <p>l2 损失和 LPIPS 损失的线性组合，权重分别为 1 和 0.05 。</p>

                <p><img src="../paper_notes/image%209.png" alt="image.png"></p>

                <h2>贡献/成果</h2>

                <p><strong>在 MVSplat 里，cost volume（代价体）可以理解成给"每张参考图像"构建的一座 H × W × D 三维记分册：</strong></p>

                <ol>
                    <li><strong>它记录了跨视图特征在不同深度假设上的相似度</strong>。
                        <ul>
                            <li>先在近-远深度范围内均匀取 D 个候选深度平面；对每一平面，把其他视图的特征按相机矩阵重投影到参考视图上。</li>
                            <li>对重投影后的特征与参考视图特征做点积相关，得到一张"相似度图"。</li>
                            <li>这样就为参考图像里的每个像素留下了 D 个"匹配分数"；把这 D 层堆叠起来，就得到尺寸 H/4 × W/4 × D 的 cost volume（论文后续再上采到全分辨率）。</li>
                        </ul>
                    </li>
                    <li><strong>它的作用是把"几张图片之间该像素真正处于哪一深度"的信息显式交给网络</strong>。高相似度意味着这些视图在该深度上观测到同一表面，因此 cost volume 为后续深度回归提供强几何线索。</li>
                    <li><strong>后端处理方式</strong>
                        <ul>
                            <li>MVSplat 把 cost volume 与经过Transformer得到的跨视图特征在通道维拼接，送入轻量 2D U-Net 做细化；再对深度维做 softmax、求期望，直接输出稠密深度图。</li>
                            <li>如果 cost volume 被移除，几何质量和渲染指标都会大幅下降，论文的消融实验专门验证了这一点。</li>
                        </ul>
                    </li>
                </ol>

                <p><strong>一句话</strong>：在这篇文章中，cost volume 就是把"<em>这个像素在不同深度平面上的跨视图一致性</em>"编码成一个三维张量，网络只需在里面寻找最低代价（最高相似度）的位置，就能迅速推断出准确深度，从而为 3D 高斯中心定位打下坚实基础。</p>

                <h3>① Multi-View Transformer 是不是作者首创？</h3>

                <p>不是。把 <strong>Self-Attention + Cross-View Attention</strong> 用在多视图立体里，最早可追溯到 <strong>TransMVSNet</strong>（ICCV'21→CVPR'22）——该工作提出"Feature-Matching Transformer"，同样用自注意力聚合图内上下文、再用跨视注意力聚合其它视图特征。MVSplat 延续了这一思想，只是把核心块换成轻量 <strong>Swin-Transformer 窗口注意力</strong> 并减少层数，以减小显存并保持实时推理；因此 <strong>"跨视 Transformer" 不是新算法，而是对现有方案的工程化精简</strong>，真正的新点在后端一次前向生成 3D Gaussians。</p>

                <h3>② 除了纯光度（L2 + LPIPS）还能加哪些无监督损失？</h3>

                <table>
                    <thead>
                        <tr>
                            <th>额外损失</th>
                            <th>作用</th>
                            <th>常见做法 / 文献</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>SSIM 结构相似度</strong></td>
                            <td>减少曝光差带来的像素级抖动；与 L1/L2 并用</td>
                            <td>CL-MVSNet 在 photometric 损失里同时加 SSIM 项</td>
                        </tr>
                        <tr>
                            <td><strong>深度平滑 / TV 正则</strong></td>
                            <td>约束相邻像素深度差，抑制噪声</td>
                            <td>无监督 MVS 与单目估深普遍使用 edge-aware smooth loss</td>
                        </tr>
                        <tr>
                            <td><strong>梯度一致性</strong></td>
                            <td>强调图像梯度在重投影后也要一致，改善纹理边缘对齐</td>
                            <td>"First-order gradient consistency" 于 UNMVS-TIJCV'21 提出</td>
                        </tr>
                        <tr>
                            <td><strong>前向-后向几何一致</strong></td>
                            <td>利用已估深度把源图反投影，再与参考视图重投影求差，过滤遮挡</td>
                            <td>多数无监督 MVS 框架（GeoMVSNet、DIV-Loss 等）含此项</td>
                        </tr>
                        <tr>
                            <td><strong>视图选择 / 像素置信度</strong></td>
                            <td>学习权重或掩码，忽略遮挡与外极面</td>
                            <td>早期 MVSNet 族用 <strong>pixel-wise confidence</strong> 或 <strong>focal/ambiguity loss</strong> 来降低误匹配</td>
                        </tr>
                    </tbody>
                </table>

                <blockquote>
                    <p>在 MVSplat 作者的实验里，这些附加约束会提升一点边缘精度，但会降低整体运行速度；为了保持 端到端 0.1 s 推理，论文最终只保留了 L2 + 0.05·LPIPS 两项光度误差，并在消融中说明加 Smooth/SSIM 收益有限而耗时显著增加。</p>
                </blockquote>

                <ul>
                    <li><strong>训练损失</strong>：只用 <strong>L2 + 0.05·LPIPS</strong> 对比渲染图与真图，就能端到端学出深度和全部高斯参数，省去人工深度标注。</li>
                    <li><strong>Multi-View Transformer</strong>：在每层加入 self + cross-view attention，用 Swin 窗口高效交换视图信息，奠定后续几何一致基础。</li>
                </ul>

                <h3>一句话总结</h3>

                <blockquote>
                    <p>为什么能只用光度误差？</p>
                    <ul>
                        <li>通过上游 <strong>cost volume + Transformer</strong>，网络已把"对齐几何"线索嵌进特征；</li>
                        <li>3D GS 渲染可直接微分到深度和高斯参数；</li>
                        <li>因此只要让渲染结果在像素和感知两层与真图一致，网络就能同时收敛几何和外观，无需任何 GT 深度或法线。</li>
                    </ul>
                </blockquote>

                <table>
                    <thead>
                        <tr>
                            <th>项</th>
                            <th>公式/实现</th>
                            <th>目的</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>像素级 L2（论文记 ℓ2）</strong></td>
                            <td>对渲染图 Ĩ 与目标 RGB 图 I 逐像素求平方差并取平均；权重 <strong>1.0</strong></td>
                            <td>约束整体亮度、颜色一致</td>
                        </tr>
                        <tr>
                            <td><strong>LPIPS 感知损失</strong></td>
                            <td>用预训练 VGG16 等 backbone，在多层特征空间计算欧氏距离；论文权重 <strong>0.05</strong></td>
                            <td>捕捉纹理/结构相似度，弥补 L2 对高频不敏感的缺陷</td>
                        </tr>
                        <tr>
                            <td><strong>总损失</strong></td>
                            <td><strong>L = L2 + 0.05 × LPIPS</strong>(论文未使用 L1；若口头表述为 "L1"，实为经典像素重建项，此处实现是 L2)</td>
                            <td>仅靠可微渲染图与真实图对比即可反向传播，<strong>完全不需要深度 GT</strong>；与 pixelSplat 相比省去额外标签</td>
                        </tr>
                    </tbody>
                </table>

                <h3>2. 纯光度损失（L2 + LPIPS）的组成与原因</h3>

                <p>Self-attention保证每张图自身的纹理连贯，Cross-attention让同一物点在不同视图的特征"互相对焦"，从而教网络"这些 patch 来自同一三维点"，为后面的匹配/深度回归提供强先验。</p>

                <h3>直观理解</h3>

                <table>
                    <thead>
                        <tr>
                            <th>子模块</th>
                            <th>具体做法</th>
                            <th>作用 / 论文依据</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>特征输入</strong></td>
                            <td>每张图先经 6 层浅 ResNet 下采到 ¼ 分辨率 (H/4 × W/4)</td>
                            <td>提取初步局部纹理</td>
                        </tr>
                        <tr>
                            <td><strong>堆叠结构</strong></td>
                            <td><strong>6 个 Transformer block</strong>，每块顺序是  ① <strong>Self-Attention</strong>（只看本视图 patch） ② <strong>Cross-View Attention</strong>（把本视图 token 当 "query"，其他视图 token 当 "key / value"）</td>
                            <td>Self 部分巩固自身上下文；Cross 部分显式交换多视图信息</td>
                        </tr>
                        <tr>
                            <td><strong>注意力窗</strong></td>
                            <td>用 <strong>Swin-Transformer</strong> 的 2 × 2 局部窗口机制，对长图高效；窗口划分对所有视图一致，保证对齐</td>
                            <td>既保局部分辨率，又控制计算量</td>
                        </tr>
                        <tr>
                            <td><strong>视图数适配</strong></td>
                            <td>Cross-attention 对"其余全部视图"做一次性聚合，权重矩阵与视图数量无关，可在推理时随意增减输入视图</td>
                            <td>训练 2-view，推理 N-view 也能用</td>
                        </tr>
                        <tr>
                            <td><strong>下游传播</strong></td>
                            <td>得到的跨视一致特征 <strong>Fi</strong> 会与各自的 cost volume 拼接送入 U-Net；U-Net 内部最低分辨率层再插入 3 个 cross-view attention 层细化体数据</td>
                            <td>把跨视信息继续输往深度估计和高斯属性预测</td>
                        </tr>
                        <tr>
                            <td><strong>有效性</strong></td>
                            <td>去掉 cross-view attention，PSNR ↓1 dB、LPIPS↑，且训练过拟合更严重</td>
                            <td>证明跨视信息流动是几何学习关键</td>
                        </tr>
                    </tbody>
                </table>

                <h3>1. Multi-View Transformer（带跨视注意力）的细节</h3>

                <h3>L1 和 L2 损失的区别与作用</h3>

                <p>这两种损失函数都属于「重建类损失」，在深度学习中用来衡量预测值和真实值之间的差异，尤其常用于图像生成、回归任务（如重建深度图、RGB图）中。</p>

                <h4>🔹 <strong>L1 Loss</strong>（绝对误差）</h4>

                <p><strong>定义</strong>：</p>

                <p>L1=1N∑i=1N∣y^i−yi∣</p>

                <p><strong>直观含义</strong>：每个像素预测错多少就罚多少（线性增长）</p>

                <p><strong>优点</strong>：</p>

                <ul>
                    <li>更鲁棒于噪声和 outlier；</li>
                    <li>更保边缘（因为梯度恒定）；</li>
                </ul>

                <p><strong>缺点</strong>：</p>

                <ul>
                    <li>不可导于 0 处（但可用伪导数解决）；</li>
                    <li>收敛速度可能慢。</li>
                </ul>

                <h4>🔹 <strong>L2 Loss</strong>（平方误差）</h4>

                <p><strong>定义</strong>：</p>

                <p>L2=1N∑i=1N(y^i−yi)2</p>

                <p><strong>直观含义</strong>：预测越错，惩罚越大（平方增长）</p>

                <p><strong>优点</strong>：</p>

                <ul>
                    <li>对小误差收敛快；</li>
                    <li>可导且梯度平滑，优化稳定；</li>
                </ul>

                <p><strong>缺点</strong>：</p>

                <ul>
                    <li>对异常值非常敏感（大误差平方后变很大）；</li>
                    <li>容易模糊图像（因为更追求整体均匀）</li>
                </ul>

                <h4>📌 在 MVSplat 或图像生成中的选择：</h4>

                <table>
                    <thead>
                        <tr>
                            <th>场景</th>
                            <th>常用损失</th>
                            <th>原因</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>渲染图与真实图的像素对比</td>
                            <td>通常 <strong>L2 或 L1 + LPIPS</strong></td>
                            <td>L2 快速收敛，LPIPS 保结构纹理</td>
                        </tr>
                        <tr>
                            <td>边缘锐化、更注重轮廓</td>
                            <td>偏向 <strong>L1</strong></td>
                            <td>抑制图像模糊，更适合重建边界清晰图</td>
                        </tr>
                        <tr>
                            <td>存在较多 outlier（遮挡、运动）</td>
                            <td>L1 更鲁棒</td>
                            <td>L2 会放大错误点的影响</td>
                        </tr>
                    </tbody>
                </table>

                <h4>总结对比：</h4>

                <table>
                    <thead>
                        <tr>
                            <th>比较项</th>
                            <th>L1</th>
                            <th>L2</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>损失增长</td>
                            <td>线性</td>
                            <td>平方</td>
                        </tr>
                        <tr>
                            <td>对 outlier</td>
                            <td>稳定</td>
                            <td>敏感</td>
                        </tr>
                        <tr>
                            <td>收敛速度</td>
                            <td>稍慢</td>
                            <td>快</td>
                        </tr>
                        <tr>
                            <td>图像效果</td>
                            <td>较清晰</td>
                            <td>易模糊</td>
                        </tr>
                        <tr>
                            <td>是否可导</td>
                            <td>不可导于 0（可近似）</td>
                            <td>可导且平滑</td>
                        </tr>
                    </tbody>
                </table>

                <blockquote>
                    <p>所以实际工程中常常 L1 + LPIPS / L2 + LPIPS 混合使用，以平衡结构保持和色彩匹配，MVSplat 就用了后者。</p>
                </blockquote>
            </div>
        </article>
    </div>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <div class="footer-links">
                <a href="mailto:Dong48@illinois.edu" aria-label="Send email to Jiacheng Dong">Email</a>
                <a href="https://github.com/dongjiacheng06" target="_blank" rel="noopener" aria-label="Visit GitHub profile">GitHub</a>
                <a href="../index.html" aria-label="Visit homepage">主页</a>
                <a href="../blog.html" aria-label="Visit blog page">博客</a>
            </div>
            <div>&copy; 2025 Jiacheng Dong. All rights reserved.</div>
            <div style="margin-top:0.5rem; font-size:0.9rem; opacity:0.8;">
                Personal Website: <a href="https://dongjiacheng06.github.io" target="_blank" rel="noopener" style="color:#fff;" aria-label="Visit personal website">dongjiacheng06.github.io</a>
            </div>
        </div>
    </footer>
</body>
</html>